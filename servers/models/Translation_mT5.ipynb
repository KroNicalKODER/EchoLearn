{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7525397,"sourceType":"datasetVersion","datasetId":4383367}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:12:20.430224Z","iopub.execute_input":"2025-03-05T12:12:20.430506Z","iopub.status.idle":"2025-03-05T12:12:20.648586Z","shell.execute_reply.started":"2025-03-05T12:12:20.430482Z","shell.execute_reply":"2025-03-05T12:12:20.647783Z"}},"outputs":[{"name":"stdout","text":"Wed Mar  5 12:12:20 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   41C    P8             11W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   41C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install transformers datasets nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T15:41:26.677392Z","iopub.execute_input":"2025-03-05T15:41:26.677706Z","iopub.status.idle":"2025-03-05T15:41:31.825275Z","shell.execute_reply.started":"2025-03-05T15:41:26.677684Z","shell.execute_reply":"2025-03-05T15:41:31.824467Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport sys\nimport transformers\nimport tensorflow as tf\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\nfrom transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport wandb\nwandb.init(mode=\"disabled\")\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:50:06.478179Z","iopub.execute_input":"2025-03-05T17:50:06.478490Z","iopub.status.idle":"2025-03-05T17:50:12.322194Z","shell.execute_reply.started":"2025-03-05T17:50:06.478460Z","shell.execute_reply":"2025-03-05T17:50:12.321434Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"model_checkpoint = 'snehalyelmati/mt5-hindi-to-english'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:50:15.850558Z","iopub.execute_input":"2025-03-05T17:50:15.851036Z","iopub.status.idle":"2025-03-05T17:50:15.855994Z","shell.execute_reply.started":"2025-03-05T17:50:15.850995Z","shell.execute_reply":"2025-03-05T17:50:15.854892Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#import dataset\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"yash9439/eng-hinglish-machine-translation\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T15:41:45.653957Z","iopub.execute_input":"2025-03-05T15:41:45.654241Z","iopub.status.idle":"2025-03-05T15:41:45.871591Z","shell.execute_reply.started":"2025-03-05T15:41:45.654213Z","shell.execute_reply":"2025-03-05T15:41:45.870819Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/eng-hinglish-machine-translation\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def read_file(file_path):\n    with open(file_path, \"r\",encoding='utf-8') as f:\n        lines = f.read()\n    return lines\n\ntest_lines = read_file('/kaggle/input/eng-hinglish-machine-translation/Eng - Hinglish/test.txt')\ntrain_lines = read_file('/kaggle/input/eng-hinglish-machine-translation/Eng - Hinglish/train.txt')\ndev_lines = read_file('/kaggle/input/eng-hinglish-machine-translation/Eng - Hinglish/dev.txt')\n\ntest_array = test_lines.split(\"\\n\")[:-1]\ntrain_array = train_lines.split(\"\\n\")[:-1]\ndev_array = dev_lines.split(\"\\n\")[:-1]\n\ntest_split = [line.split(\"\\t\") for line in test_array]\ntrain_split = [line.split(\"\\t\") for line in train_array]\ndev_split = [line.split(\"\\t\") for line in dev_array]\n\ndf = {}\n\ndf['test'] = pd.DataFrame(test_split, columns = [\"output\", \"input\"])\ndf['train'] = pd.DataFrame(train_split, columns = [\"output\", \"input\"])\ndf['dev'] = pd.DataFrame(dev_split, columns = [\"output\", \"input\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:50:22.075033Z","iopub.execute_input":"2025-03-05T17:50:22.075368Z","iopub.status.idle":"2025-03-05T17:50:22.163120Z","shell.execute_reply.started":"2025-03-05T17:50:22.075339Z","shell.execute_reply":"2025-03-05T17:50:22.162428Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from datasets import Dataset, load_dataset, DatasetDict\n\nds = DatasetDict()\nds['test'] = Dataset.from_pandas(df['test'])\nds['train'] = Dataset.from_pandas(df['train'])\nds['dev'] = Dataset.from_pandas(df['dev'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:50:25.528986Z","iopub.execute_input":"2025-03-05T17:50:25.529308Z","iopub.status.idle":"2025-03-05T17:50:25.583775Z","shell.execute_reply.started":"2025-03-05T17:50:25.529284Z","shell.execute_reply":"2025-03-05T17:50:25.582783Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T15:41:45.939592Z","iopub.execute_input":"2025-03-05T15:41:45.939881Z","iopub.status.idle":"2025-03-05T15:41:45.945618Z","shell.execute_reply.started":"2025-03-05T15:41:45.939851Z","shell.execute_reply":"2025-03-05T15:41:45.944931Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['output', 'input'],\n        num_rows: 942\n    })\n    train: Dataset({\n        features: ['output', 'input'],\n        num_rows: 7040\n    })\n    dev: Dataset({\n        features: ['output', 'input'],\n        num_rows: 1020\n    })\n})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# TOKENIZER\n\ntokenizer = AutoTokenizer.from_pretrained(\"snehalyelmati/mt5-hindi-to-english\")\n\ntokenizer.model_max_length = 32\n\ndef tokenise_df(df_obj):\n    return tokenizer.prepare_seq2seq_batch(src_texts = df_obj[\"input\"], tgt_texts = df_obj[\"output\"])\n\ntokenized_datasets = ds.map(tokenise_df, batched = True)\n\ntokenized_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:50:28.189963Z","iopub.execute_input":"2025-03-05T17:50:28.190243Z","iopub.status.idle":"2025-03-05T17:50:31.943473Z","shell.execute_reply.started":"2025-03-05T17:50:28.190219Z","shell.execute_reply":"2025-03-05T17:50:31.942486Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1145157c2dcb4418b99a6a72d3aa3edf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcd2b1d4c62f4158adf7c8923fd2794b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a84e6115a9ef4421a48c986f43b2c499"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0ea989a73a46c0a1db8b857942f9fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/942 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa58e9c822d64fee8f6672ef27277ed5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7040 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1fe01a4bc6a4535af9701eff77c9f71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1020 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dce6fdf745b4549b1dbd1502aedf795"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['output', 'input', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 942\n    })\n    train: Dataset({\n        features: ['output', 'input', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 7040\n    })\n    dev: Dataset({\n        features: ['output', 'input', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 1020\n    })\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# SETTING UP MODEL\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"snehalyelmati/mt5-hindi-to-english\")\ndata_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:50:31.944639Z","iopub.execute_input":"2025-03-05T17:50:31.944959Z","iopub.status.idle":"2025-03-05T17:50:58.522665Z","shell.execute_reply.started":"2025-03-05T17:50:31.944931Z","shell.execute_reply":"2025-03-05T17:50:58.521577Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02cc9b0bbbb641ccab2a5580e19d071f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed1c2a7460034c00b079296b443e696e"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import nltk\nfrom nltk.translate.bleu_score import sentence_bleu\n\ndef compute_metrics(pred):\n    references = pred.label_ids\n    generated_texts = pred.predictions\n    \n    bleu_scores = []\n    for reference, generated_text in zip(references, generated_texts):\n        reference_text = train_dataset[reference]['text']\n        bleu_score = sentence_bleu([reference_text], generated_text)\n        bleu_scores.append(bleu_score)\n\n    return {\n        'bleu': sum(bleu_scores) / len(bleu_scores)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:51:49.512050Z","iopub.execute_input":"2025-03-05T17:51:49.512381Z","iopub.status.idle":"2025-03-05T17:51:49.517934Z","shell.execute_reply.started":"2025-03-05T17:51:49.512348Z","shell.execute_reply":"2025-03-05T17:51:49.516869Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:51:54.000826Z","iopub.execute_input":"2025-03-05T17:51:54.001250Z","iopub.status.idle":"2025-03-05T17:51:54.439478Z","shell.execute_reply.started":"2025-03-05T17:51:54.001211Z","shell.execute_reply":"2025-03-05T17:51:54.438698Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"MT5ForConditionalGeneration(\n  (shared): Embedding(250112, 512)\n  (encoder): MT5Stack(\n    (embed_tokens): Embedding(250112, 512)\n    (block): ModuleList(\n      (0): MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): MT5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): MT5Stack(\n    (embed_tokens): Embedding(250112, 512)\n    (block): ModuleList(\n      (0): MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerCrossAttention(\n            (EncDecAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerCrossAttention(\n            (EncDecAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): MT5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"/kaggle/working/results\",\n    eval_strategy=\"epoch\",  # Evaluates at the end of every epoch\n    logging_strategy=\"epoch\",  # Logs metrics at the end of every epoch\n    num_train_epochs=20,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=4,\n    fp16=True,\n    gradient_accumulation_steps=8,\n    learning_rate=0.001,\n    # load_best_model_at_end=True,\n    push_to_hub=False,\n    save_total_limit=2,\n    save_strategy=\"epoch\",\n    logging_dir=\"/kaggle/working/logs\",\n    resume_from_checkpoint=True,\n    \n)\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"dev\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:52:27.801351Z","iopub.execute_input":"2025-03-05T17:52:27.801733Z","iopub.status.idle":"2025-03-05T17:52:28.057331Z","shell.execute_reply.started":"2025-03-05T17:52:27.801703Z","shell.execute_reply":"2025-03-05T17:52:28.056667Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# !df -h  # Check disk space usage\n# !rm -rf /kaggle/working/results/checkpoint-*  # Delete old checkpoints\n# !rm -rf /kaggle/working/logs  # Clear logs if not needed\n!rm -rf /kaggle/working/state.db","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:30:16.007488Z","iopub.execute_input":"2025-03-05T16:30:16.007825Z","iopub.status.idle":"2025-03-05T16:30:16.333195Z","shell.execute_reply.started":"2025-03-05T16:30:16.007801Z","shell.execute_reply":"2025-03-05T16:30:16.331923Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:31:32.935164Z","iopub.execute_input":"2025-03-05T16:31:32.935525Z","iopub.status.idle":"2025-03-05T17:12:52.794563Z","shell.execute_reply.started":"2025-03-05T16:31:32.935489Z","shell.execute_reply":"2025-03-05T17:12:52.793805Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1100' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1100/1100 41:16, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>55</td>\n      <td>2.350900</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.351100</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>1.154400</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.039400</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.957700</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.888500</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>0.829800</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.780800</td>\n    </tr>\n    <tr>\n      <td>495</td>\n      <td>0.736700</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.697000</td>\n    </tr>\n    <tr>\n      <td>605</td>\n      <td>0.666300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.634900</td>\n    </tr>\n    <tr>\n      <td>715</td>\n      <td>0.611000</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.581900</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>0.563600</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.545800</td>\n    </tr>\n    <tr>\n      <td>935</td>\n      <td>0.530400</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.521400</td>\n    </tr>\n    <tr>\n      <td>1045</td>\n      <td>0.511100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.506300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1100, training_loss=0.8229520988464355, metrics={'train_runtime': 2478.9449, 'train_samples_per_second': 56.798, 'train_steps_per_second': 0.444, 'total_flos': 4652997279744000.0, 'train_loss': 0.8229520988464355, 'epoch': 20.0})"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n\ntraining_args.per_device_eval_batch_size = 8\ntraining_args.eval_accumulation_steps = 8  \n\nresults = trainer.evaluate()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:12:53.777629Z","iopub.status.idle":"2025-03-05T17:12:53.778011Z","shell.execute_reply":"2025-03-05T17:12:53.777841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## PREDICTION\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ncheckpoint_path = \"/kaggle/working/results/checkpoint-1100\"  # Update with your path\n\n# Load the model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n\n# Move model to GPU if available\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:02:24.998198Z","iopub.execute_input":"2025-03-05T18:02:24.998498Z","iopub.status.idle":"2025-03-05T18:02:30.685946Z","shell.execute_reply.started":"2025-03-05T18:02:24.998474Z","shell.execute_reply":"2025-03-05T18:02:30.684948Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"MT5ForConditionalGeneration(\n  (shared): Embedding(250112, 512)\n  (encoder): MT5Stack(\n    (embed_tokens): Embedding(250112, 512)\n    (block): ModuleList(\n      (0): MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): MT5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): MT5Stack(\n    (embed_tokens): Embedding(250112, 512)\n    (block): ModuleList(\n      (0): MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerCrossAttention(\n            (EncDecAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerCrossAttention(\n            (EncDecAttention): MT5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): MT5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"text = \"मैं जिज्ञासा से इसे बनाने के लिए कहूंगा\"  # Replace with actual input\n\n# Tokenize the input\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:56:10.481077Z","iopub.execute_input":"2025-03-05T18:56:10.481384Z","iopub.status.idle":"2025-03-05T18:56:10.486279Z","shell.execute_reply.started":"2025-03-05T18:56:10.481361Z","shell.execute_reply":"2025-03-05T18:56:10.485314Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Generate response\nwith torch.no_grad():\n    output_ids = model.generate(**inputs, max_length=64)  # Adjust `max_length` as needed\n\n# Decode the generated response\nresponse = tokenizer.decode(output_ids[0], skip_special_tokens=True)\nprint(\"Generated Response:\", response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:58:46.870982Z","iopub.execute_input":"2025-03-05T18:58:46.871346Z","iopub.status.idle":"2025-03-05T18:58:47.134461Z","shell.execute_reply.started":"2025-03-05T18:58:46.871318Z","shell.execute_reply":"2025-03-05T18:58:47.133707Z"}},"outputs":[{"name":"stdout","text":"Generated Response: I would say it would be a good message to make it\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"## DOWNLOADING THE CHECKPOINT FOLDER\n!zip -r file_logs.zip /kaggle/working/logs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:38:56.911756Z","iopub.execute_input":"2025-03-05T18:38:56.912093Z","iopub.status.idle":"2025-03-05T18:38:57.185147Z","shell.execute_reply.started":"2025-03-05T18:38:56.912067Z","shell.execute_reply":"2025-03-05T18:38:57.184131Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/logs/ (stored 0%)\n  adding: kaggle/working/logs/events.out.tfevents.1741192274.0cde151efdd7.1428.3 (deflated 61%)\n  adding: kaggle/working/logs/events.out.tfevents.1741192293.0cde151efdd7.1428.4 (deflated 62%)\n  adding: kaggle/working/logs/events.out.tfevents.1741192228.0cde151efdd7.1428.2 (deflated 61%)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file_logs.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:39:06.892120Z","iopub.execute_input":"2025-03-05T18:39:06.892454Z","iopub.status.idle":"2025-03-05T18:39:06.898866Z","shell.execute_reply.started":"2025-03-05T18:39:06.892424Z","shell.execute_reply":"2025-03-05T18:39:06.898078Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file_logs.zip","text/html":"<a href='file_logs.zip' target='_blank'>file_logs.zip</a><br>"},"metadata":{}}],"execution_count":27}]}